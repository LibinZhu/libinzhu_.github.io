---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---


## Feature learning
- L Zhu, C Liu, A Radhakrishnan, M Belkin, [Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning](https://arxiv.org/pdf/2306.04815.pdf). ICML 2024.
- L Zhu, C Liu, A Radhakrishnan, M Belkin, [Quadratic models for understanding catapult dynamics of neural networks](https://arxiv.org/pdf/2205.11787.pdf). ICLR 2024.
- L Zhu, P Pandit, M Belkin, [A note on Linear Bottleneck networks and their Transition to Multilinearity](https://arxiv.org/pdf/2206.15058.pdf). Pre-printed.

## Kernel regime
- A Banerjee, P Cisneros-Velarde, L Zhu, M Belkin, [Neural tangent kernel at initialization: linear width suffices](https://proceedings.mlr.press/v216/banerjee23a/banerjee23a.pdf). UAI 2023.
- A Banerjee, P Cisneros-Velarde, L Zhu, M Belkin, [Restricted Strong Convexity of Deep Learning Models with Smooth Activations](https://arxiv.org/pdf/2209.15106.pdf). ICLR 2023.
- L Zhu, C Liu, M Belkin, [Transition to Linearity of General Neural Networks with Directed Acyclic Graph Architecture](https://arxiv.org/pdf/2205.11786.pdf). NeurIPS 2022. 
- C Liu, L Zhu, M Belkin, [Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models](https://arxiv.org/pdf/2203.05104.pdf). ICLR 2021 (Spotlight). 
- C Liu, L Zhu, M Belkin, [On the linearity of large non-linear models: when and why the tangent kernel is constant](https://arxiv.org/pdf/2010.01092.pdf). NeurIPS 2020 (Spotlight). 
- C Liu, L Zhu, M Belkin, [Loss landscapes and optimization in over-parameterized non-linear systems and neural networks](https://arxiv.org/pdf/2003.00307.pdf). Applied and Computational Harmonic Analysis (ACHA) 2022.
